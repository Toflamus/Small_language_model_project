{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is the merged code for Finetune process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available:  True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA Available: \", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Settings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import logging\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, Optional, Sequence\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "import utils\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import Trainer\n",
    "\n",
    "# import wandb\n",
    "# wandb.login(relogin=True)\n",
    "\n",
    "\n",
    "\n",
    "IGNORE_INDEX = -100\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "DEFAULT_EOS_TOKEN = \"</s>\"\n",
    "DEFAULT_BOS_TOKEN = \"<s>\"\n",
    "DEFAULT_UNK_TOKEN = \"<unk>\"\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\"\n",
    "    ),\n",
    "    \"prompt_no_input\": (\n",
    "        \"Below is an instruction that describes a task. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Response:\"\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArguments:\n",
    "    model_name_or_path: str = \"Toflamus/GPT-2_para3M\"\n",
    "    # Add other model-related arguments here\n",
    "\n",
    "@dataclass\n",
    "class DataArguments:\n",
    "    data_path: str = \"/workspace/lost+found/Final_Project/alpaca_data.json\"\n",
    "    # Add other data-related arguments here\n",
    "\n",
    "@dataclass\n",
    "class TrainingArguments:\n",
    "    model_max_length: int = 512\n",
    "    cache_dir: str = \"/workspace/lost+found/Final_Project/cache\"  # Set your desired cache directory\n",
    "    bf16: bool = True\n",
    "    output_dir: str = '/workspace/lost+found/Final_Project/Output'\n",
    "    num_train_epochs: int = 3\n",
    "    gradient_accumulation_steps: int = 8\n",
    "    # evaluation_strategy: str = \"no\"\n",
    "    save_strategy: str = \"steps\"\n",
    "    save_steps: int = 2000\n",
    "    save_total_limit: int = 1\n",
    "    learning_rate: float = 2e-5\n",
    "    weight_decay: float = 0.0\n",
    "    warmup_ratio: float = 0.03\n",
    "    lr_scheduler_type: str = \"cosine\"\n",
    "    logging_steps: int = 1\n",
    "    # fsdp: str = \"full_shard auto_wrap\"\n",
    "    # fsdp_transformer_layer_cls_to_wrap: str = 'LlamaDecoderLayer'\n",
    "    tf32: bool = True\n",
    "    full_determinism = False \n",
    "    seed = 42\n",
    "    accelerate_version = 1\n",
    "\n",
    "# Create instances of the dataclasses with your desired parameter values\n",
    "model_args = ModelArguments()\n",
    "data_args = DataArguments()\n",
    "training_args = TrainingArguments()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    cache_dir=training_args.cache_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    cache_dir=training_args.cache_dir,\n",
    "    model_max_length=training_args.model_max_length,\n",
    "    padding_side=\"right\",\n",
    "    use_fast=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define special tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens_dict = dict()\n",
    "if tokenizer.pad_token is None:\n",
    "    special_tokens_dict[\"pad_token\"] = DEFAULT_PAD_TOKEN\n",
    "if tokenizer.eos_token is None:\n",
    "    special_tokens_dict[\"eos_token\"] = DEFAULT_EOS_TOKEN\n",
    "if tokenizer.bos_token is None:\n",
    "    special_tokens_dict[\"bos_token\"] = DEFAULT_BOS_TOKEN\n",
    "if tokenizer.unk_token is None:\n",
    "    special_tokens_dict[\"unk_token\"] = DEFAULT_UNK_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load preprocessing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some functions to load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "# import logging\n",
    "import math\n",
    "import os\n",
    "import io\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "from typing import Optional, Sequence, Union\n",
    "\n",
    "def _make_w_io_base(f, mode: str):\n",
    "    if not isinstance(f, io.IOBase):\n",
    "        f_dirname = os.path.dirname(f)\n",
    "        if f_dirname != \"\":\n",
    "            os.makedirs(f_dirname, exist_ok=True)\n",
    "        f = open(f, mode=mode)\n",
    "    return f\n",
    "\n",
    "def _make_r_io_base(f, mode: str):\n",
    "    if not isinstance(f, io.IOBase):\n",
    "        f = open(f, mode=mode)\n",
    "    return f\n",
    "\n",
    "\n",
    "def jdump(obj, f, mode=\"w\", indent=4, default=str):\n",
    "    \"\"\"Dump a str or dictionary to a file in json format.\n",
    "\n",
    "    Args:\n",
    "        obj: An object to be written.\n",
    "        f: A string path to the location on disk.\n",
    "        mode: Mode for opening the file.\n",
    "        indent: Indent for storing json dictionaries.\n",
    "        default: A function to handle non-serializable entries; defaults to `str`.\n",
    "    \"\"\"\n",
    "    f = _make_w_io_base(f, mode)\n",
    "    if isinstance(obj, (dict, list)):\n",
    "        json.dump(obj, f, indent=indent, default=default)\n",
    "    elif isinstance(obj, str):\n",
    "        f.write(obj)\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected type: {type(obj)}\")\n",
    "    f.close()\n",
    "\n",
    "\n",
    "def jload(f, mode=\"r\"):\n",
    "    \"\"\"Load a .json file into a dictionary.\"\"\"\n",
    "    f = _make_r_io_base(f, mode)\n",
    "    jdict = json.load(f)\n",
    "    f.close()\n",
    "    return jdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Loading data...\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "data_path = \"/workspace/lost+found/Final_Project/alpaca_data.json\"\n",
    "train_path = \"/workspace/lost+found/Final_Project/alpaca_train.json\"\n",
    "test_path = \"/workspace/lost+found/Final_Project/alpaca_test.json\"\n",
    "eval_data = load_dataset(\"json\", data_files = train_path)\n",
    "# print(eval_data)\n",
    "logging.warning(\"Loading data...\")\n",
    "# list_data_dict = train_data[\"train\"].to_dict()\n",
    "# Load a dataset\n",
    "train_data = load_dataset(\"json\", data_files = test_path)\n",
    "# Custom function to format the dataset\n",
    "def custom_format(entry):\n",
    "    return {\n",
    "        'instruction': entry['instruction'],\n",
    "        'input': entry['input'] if 'input' in entry else '',\n",
    "        'output': entry['output']\n",
    "    }\n",
    "\n",
    "# Convert the dataset using the custom format function\n",
    "\n",
    "# list_data_dict = [custom_format(train_data[\"train\"][i]) for i in range(train_data[\"train\"].num_rows)]\n",
    "# list_eval_dict = [custom_format(eval_data[\"train\"][i]) for i in range(eval_data[\"train\"].num_rows)]\n",
    "# # list_data_dict = jload(data_path)\n",
    "# # list_data_dict = train_data\n",
    "# list_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Formatting inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging.warning(\"Formatting inputs...\")\n",
    "# prompt_input, prompt_no_input = PROMPT_DICT[\"prompt_input\"], PROMPT_DICT[\"prompt_no_input\"]\n",
    "# # print(prompt_input)\n",
    "# # print(prompt_no_input)\n",
    "# sources = [\n",
    "#     prompt_input.format_map(example) if example.get(\"input\", \"\") != \"\" else prompt_no_input.format_map(example)\n",
    "#     for example in list_data_dict\n",
    "# ]# here we use the template to generate the sources\n",
    "# targets = [f\"{example['output']}{tokenizer.eos_token}\" for example in list_data_dict]\n",
    "# print(sources[:2])\n",
    "# print(targets[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizing inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _tokenize_fn(strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n",
    "    \"\"\"Tokenize a list of strings. This can only tokenize a single string\"\"\"\n",
    "    tokenized_list = [\n",
    "        tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"longest\",\n",
    "            max_length=tokenizer.model_max_length,\n",
    "            truncation=True,\n",
    "        )\n",
    "        for text in strings\n",
    "    ]\n",
    "    input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n",
    "    # here we defined a label for trainer\n",
    "    input_ids_lens = labels_lens = [\n",
    "        tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list\n",
    "    ]\n",
    "    return dict(\n",
    "        input_ids=input_ids,\n",
    "        labels=labels,\n",
    "        input_ids_lens=input_ids_lens,\n",
    "        labels_lens=labels_lens,\n",
    "    )\n",
    "\n",
    "def preprocess(\n",
    "    sources: Sequence[str],\n",
    "    targets: Sequence[str],\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    ") -> Dict:\n",
    "    \"\"\"Preprocess the data by tokenizing.\"\"\"\n",
    "    examples = [s + t for s, t in zip(sources, targets)]# combine the sentences in sources and targets\n",
    "    examples_tokenized, sources_tokenized = [_tokenize_fn(strings, tokenizer) for strings in (examples, sources)]\n",
    "    input_ids = examples_tokenized[\"input_ids\"]\n",
    "    labels = copy.deepcopy(input_ids)\n",
    "    for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n",
    "        label[:source_len] = IGNORE_INDEX\n",
    "    return dict(input_ids=input_ids, labels=labels)\n",
    "# 这里mask了一下 把label的问题部分给盖掉了\n",
    "\n",
    "# logging.warning(\"Tokenizing inputs... This may take some time...\")\n",
    "# data_dict = preprocess(sources, targets, tokenizer)\n",
    "# data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(data_dict['input_ids'][:2])\n",
    "# print(data_dict['labels'][:2])\n",
    "# print(tokenizer.decode(data_dict['input_ids'][0]))#, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resize tokenizer and embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embeding dimension will be 50257. This might induce some performance reduction as *Tensor Cores* will not be available. For more details  about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n"
     ]
    }
   ],
   "source": [
    "def smart_tokenizer_and_embedding_resize(\n",
    "    special_tokens_dict: Dict,\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    "    model: transformers.PreTrainedModel,\n",
    "):\n",
    "    \"\"\"Resize tokenizer and embedding.\n",
    "\n",
    "    Note: This is the unoptimized version that may make your embedding size not be divisible by 64.\n",
    "    \"\"\"\n",
    "    num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    if num_new_tokens > 0:\n",
    "        input_embeddings = model.get_input_embeddings().weight.data\n",
    "        output_embeddings = model.get_output_embeddings().weight.data\n",
    "\n",
    "        input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n",
    "        output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n",
    "\n",
    "        input_embeddings[-num_new_tokens:] = input_embeddings_avg\n",
    "        output_embeddings[-num_new_tokens:] = output_embeddings_avg\n",
    "\n",
    "smart_tokenizer_and_embedding_resize(\n",
    "        special_tokens_dict=special_tokens_dict,\n",
    "        tokenizer=tokenizer,\n",
    "        model=model,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define datacollector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is a Data Collator?[here](https://saturncloud.io/blog/creating-a-custom-data-collator-for-huggingface-trainer-a-comprehensive-guide/#what-is-a-data-collator)\n",
    "A data_collator is a function that takes a batch of data and collates it into a format suitable for model training. The default data_collator in HuggingFace Trainer handles most common scenarios, but there are cases where you might need a custom one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Loading data...\n",
      "WARNING:root:Formatting inputs...\n",
      "WARNING:root:Tokenizing inputs... This may take some time...\n",
      "WARNING:root:Loading data...\n",
      "WARNING:root:Formatting inputs...\n",
      "WARNING:root:Tokenizing inputs... This may take some time...\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "@dataclass\n",
    "class DataCollatorForSupervisedDataset(object):\n",
    "    \"\"\"Collate examples for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    tokenizer: transformers.PreTrainedTokenizer\n",
    "\n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=IGNORE_INDEX)\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
    "        )\n",
    "def custom_format(entry):\n",
    "    return {\n",
    "        'instruction': entry['instruction'],\n",
    "        'input': entry['input'] if 'input' in entry else '',\n",
    "        'output': entry['output']\n",
    "    }\n",
    "\n",
    "# Convert the dataset using the custom format function\n",
    "\n",
    "\n",
    "class SupervisedDataset(Dataset):\n",
    "    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    def __init__(self, data_path: str, tokenizer: transformers.PreTrainedTokenizer):\n",
    "        super(SupervisedDataset, self).__init__()\n",
    "        logging.warning(\"Loading data...\")\n",
    "        train_data = load_dataset(\"json\", data_files = data_path)\n",
    "        list_data_dict = [custom_format(train_data[\"train\"][i]) for i in range(train_data[\"train\"].num_rows)]\n",
    "#         list_data_dict = train_data[\"train\"].to_dict()\n",
    "#         There used jload and utils\n",
    "\n",
    "        # here we got the dataset\n",
    "#         list_data_dict = jload(data_path)\n",
    "        logging.warning(\"Formatting inputs...\")\n",
    "        prompt_input, prompt_no_input = PROMPT_DICT[\"prompt_input\"], PROMPT_DICT[\"prompt_no_input\"]\n",
    "        sources = [\n",
    "            prompt_input.format_map(example) if example.get(\"input\", \"\") != \"\" else prompt_no_input.format_map(example)\n",
    "            for example in list_data_dict\n",
    "        ]# here we use the template to generate the sources\n",
    "        targets = [f\"{example['output']}{tokenizer.eos_token}\" for example in list_data_dict]\n",
    "\n",
    "        logging.warning(\"Tokenizing inputs... This may take some time...\")\n",
    "        data_dict = preprocess(sources, targets, tokenizer)\n",
    "\n",
    "        self.input_ids = data_dict[\"input_ids\"]\n",
    "        self.labels = data_dict[\"labels\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        return dict(input_ids=self.input_ids[i], labels=self.labels[i])\n",
    "data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
    "train_dataset = SupervisedDataset(tokenizer=tokenizer, data_path=train_path)\n",
    "eval_dataset = SupervisedDataset(tokenizer=tokenizer,data_path=test_path)\n",
    "# train_datset = data_dict\n",
    "# preprocess evaldata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = dict(train_dataset = train_dataset, eval_dataset = eval_dataset, data_collator = data_collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### See what the dataset look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert the dataset to huggingface dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "# dataset = Dataset.from_dict(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Login the wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ········································\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login(relogin=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizing inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Login to hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git config --global user.email \"toflamus12138@gmail.com\"\n",
    "!git config --global user.name \"Toflamus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(token='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtoflamus12138\u001b[0m (\u001b[33mtoflamusteam\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.9 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/lost+found/Final_Project/wandb/run-20230831_141155-d144ebbq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/toflamusteam/huggingface/runs/d144ebbq' target=\"_blank\">solar-firebrand-14</a></strong> to <a href='https://wandb.ai/toflamusteam/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/toflamusteam/huggingface' target=\"_blank\">https://wandb.ai/toflamusteam/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/toflamusteam/huggingface/runs/d144ebbq' target=\"_blank\">https://wandb.ai/toflamusteam/huggingface/runs/d144ebbq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='209' max='1825' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 209/1825 01:33 < 12:10, 2.21 it/s, Epoch 0.57/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>7.679700</td>\n",
       "      <td>7.035451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>6.984200</td>\n",
       "      <td>6.675406</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args1 = TrainingArguments(\n",
    "    output_dir=\"/workspace/lost+found/Final_Project/Output\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    logging_steps=100,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.0,\n",
    "    warmup_steps=100,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=2e-5,\n",
    "    save_steps=1000,\n",
    "    fp16=True,\n",
    "    # push_to_hub=True,\n",
    "    report_to=\"wandb\",\n",
    "    tf32=True,\n",
    "    full_determinism=False,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args1,\n",
    "    **data_module,  \n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer.push_to_hub(\"Toflamus/Finetuned_with_eval\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !zip -r /workspace/lost+found/Final_Project/Finetune2.zip /workspace/lost+found/Final_Project/Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 4358\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 36718\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 3760\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['',\n",
       "  ' = Robert Boulter = \\n',\n",
       "  '',\n",
       "  ' Robert Boulter is an English film , television and theatre actor . He had a guest @-@ starring role on the television series The Bill in 2000 . This was followed by a starring role in the play Herons written by Simon Stephens , which was performed in 2001 at the Royal Court Theatre . He had a guest role in the television series Judge John Deed in 2002 . In 2004 Boulter landed a role as \" Craig \" in the episode \" Teddy \\'s Story \" of the television series The Long Firm ; he starred alongside actors Mark Strong and Derek Jacobi . He was cast in the 2005 theatre productions of the Philip Ridley play Mercury Fur , which was performed at the Drum Theatre in Plymouth and the Menier Chocolate Factory in London . He was directed by John Tiffany and starred alongside Ben Whishaw , Shane Zaza , Harry Kent , Fraser Ayres , Sophie Stanton and Dominic Hall . \\n',\n",
       "  ' In 2006 , Boulter starred alongside Whishaw in the play Citizenship written by Mark Ravenhill . He appeared on a 2006 episode of the television series , Doctors , followed by a role in the 2007 theatre production of How to Curse directed by Josie Rourke . How to Curse was performed at Bush Theatre in the London Borough of Hammersmith and Fulham . Boulter starred in two films in 2008 , Daylight Robbery by filmmaker Paris Leonti , and Donkey Punch directed by Olly Blackburn . In May 2008 , Boulter made a guest appearance on a two @-@ part episode arc of the television series Waking the Dead , followed by an appearance on the television series Survivors in November 2008 . He had a recurring role in ten episodes of the television series Casualty in 2010 , as \" Kieron Fletcher \" . Boulter starred in the 2011 film Mercenaries directed by Paris Leonti . \\n',\n",
       "  '']}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['test'][:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "# Load your fine-tuned model and tokenizer\n",
    "model_name = \"Toflamus/Finetuned3\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Initialize the metric for perplexity\n",
    "perplexity_metric = load_metric(\"perplexity\")\n",
    "\n",
    "# Tokenize and evaluate the dataset for perplexity\n",
    "def compute_metrics(p):\n",
    "#     input_ids = p[\"text\"]\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(p[\"text\"]).input_ids\n",
    "    loss = model(labels, labels=labels).loss\n",
    "    return perplexity_metric.compute(predictions=loss.exp(), references=[1.0])\n",
    "\n",
    "results = dataset['test'].map(compute_metrics, batched=True)\n",
    "perplexity = results[\"perplexity\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_bleu(p):\n",
    "    input_ids = p.input_ids\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(p[\"text\"]).input_ids\n",
    "    predicted_text = tokenizer.decode(model.generate(input_ids))\n",
    "    return bleu_metric.compute(predictions=predicted_text, references=p[\"text\"])\n",
    "\n",
    "results = dataset.map(compute_bleu, batched=True)\n",
    "bleu_score = results[\"bleu\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gpt2 = 'gpt2'\n",
    "mode_2 = AutoModelForCausalLM.from_pretrained(model_gpt2)\n",
    "tokenizer_gpt2 = AutoTokenizer.from_pretrained(model_gpt2)\n",
    "\n",
    "# Tokenize and evaluate the dataset for perplexity\n",
    "def compute_metrics(p):\n",
    "    input_ids = p.input_ids\n",
    "    with tokenizer_gpt2.as_target_tokenizer():\n",
    "        labels = tokenizer_gpt2(p[\"text\"]).input_ids\n",
    "    loss = model_gpt2(input_ids, labels=labels).loss\n",
    "    return perplexity_metric.compute(predictions=loss.exp(), references=[1.0])\n",
    "\n",
    "results_gpt2 = dataset.map(compute_metrics, batched=True)\n",
    "gpt2_perplexity = results_gpt2[\"perplexity\"]\n",
    "\n",
    "# Calculate BLEU score\n",
    "bleu_metric = load_metric(\"bleu\")\n",
    "\n",
    "def compute_bleu(p):\n",
    "    input_ids = p.input_ids\n",
    "    with tokenizer_gpt2.as_target_tokenizer():\n",
    "        labels = tokenizer_gpt2(p[\"text\"]).input_ids\n",
    "    predicted_text = tokenizer_gpt2.decode(model.generate(input_ids))\n",
    "    return bleu_metric.compute(predictions=predicted_text, references=p[\"text\"])\n",
    "\n",
    "results_gpt2 = dataset.map(compute_bleu, batched=True)\n",
    "gpt2_bleu = results_gpt2[\"bleu\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the metrics and their values for your model and the \"gpt2\" model\n",
    "metrics = [\"Perplexity\", \"BLEU\"]\n",
    "your_model_values = [perplexity, bleu_score]\n",
    "gpt2_values = [gpt2_perplexity, gpt2_bleu]\n",
    "\n",
    "# Create a radar plot\n",
    "fig, ax = plt.subplots(subplot_kw={'projection': 'polar'})\n",
    "angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False).tolist()\n",
    "angles += angles[:1]\n",
    "\n",
    "your_model_values += your_model_values[:1]\n",
    "gpt2_values += gpt2_values[:1]\n",
    "\n",
    "ax.fill(angles, your_model_values, 'b', alpha=0.1)\n",
    "ax.fill(angles, gpt2_values, 'r', alpha=0.1)\n",
    "\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(metrics)\n",
    "\n",
    "# Display the radar plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My Environment",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
